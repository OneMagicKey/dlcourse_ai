{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd2oXM945lOO",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "colab": {}
      },
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "outputId": "7db9ee60-e1ab-4860-c08b-8cd548e305ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "outputId": "1ece37df-5c51-45a4-a7ed-3d49526f88cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "outputId": "bce3f569-e3b0-45c3-898e-92afcb82f9b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "outputId": "2f5d3511-91d9-44ac-f73c-ef023563cee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'PRON', 'DET', 'VERB', 'PRT', 'X', 'CONJ', '.', 'NOUN', 'ADP', 'ADJ', 'NUM', 'ADV'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "outputId": "340fdf99-652f-4073-88a4-132a4f10c682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "outputId": "60e6ba3e-7843-4cc2-bebc-bc2b802a30f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "outputId": "11917f79-0d77-47e1-9884-44c049cedd4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "outputId": "4345b844-1242-41f2-c87d-c7c510cfd450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "outputId": "8a02a5e1-ea77-42c7-e162-1ceed874bc3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count)\n",
        "        self.linear = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding = self.embedding(inputs)\n",
        "        lstm_res, _  = self.lstm(embedding)\n",
        "        result = self.linear(lstm_res)\n",
        "\n",
        "        return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "outputId": "97fca477-2600-42bf-d09a-1aed7f4ac09f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "# model.eval()\n",
        "logits = model(X_batch)\n",
        "_, pred = torch.max(logits, dim=2)\n",
        "\n",
        "accuracy = float(torch.sum(pred == y_batch)) / (y_batch.shape[0] * y_batch.shape[1])\n",
        "\n",
        "print(f'Accuracy = {accuracy}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 0.0390625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "outputId": "b9863101-8b5d-4492-e97a-df3c0a4160fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = criterion(logits.reshape(-1, 13), y_batch.reshape(-1))\n",
        "print(f'Loss = {loss.item()}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 2.5464000701904297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                y_batch = y_batch.reshape(-1)\n",
        "                loss = criterion(logits.reshape(-1, 13), y_batch)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                _, pred = torch.max(logits, dim=2)\n",
        "                pred = pred.reshape(-1)\n",
        "\n",
        "                mask = torch.zeros_like(y_batch)\n",
        "                mask[y_batch != 0] = 1\n",
        "                padding = torch.sum(mask == 0)\n",
        "                pred = pred * mask\n",
        "\n",
        "                cur_correct_count, cur_sum_count = torch.sum(pred == y_batch)- padding, y_batch.shape[0] - padding\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), float(cur_correct_count) / float(cur_sum_count))\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, float(correct_count) / float(sum_count))\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, float(correct_count) / float(sum_count)\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "outputId": "c1166c7a-1157-49f4-d944-ad6ca47633df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.68602, Accuracy = 78.50%: 100%|██████████| 572/572 [00:05<00:00, 99.72it/s]\n",
            "[1 / 50]   Val: Loss = 0.36632, Accuracy = 87.57%: 100%|██████████| 13/13 [00:00<00:00, 86.71it/s]\n",
            "[2 / 50] Train: Loss = 0.27437, Accuracy = 90.88%: 100%|██████████| 572/572 [00:05<00:00, 101.71it/s]\n",
            "[2 / 50]   Val: Loss = 0.24221, Accuracy = 91.94%: 100%|██████████| 13/13 [00:00<00:00, 85.22it/s]\n",
            "[3 / 50] Train: Loss = 0.18586, Accuracy = 93.82%: 100%|██████████| 572/572 [00:05<00:00, 101.28it/s]\n",
            "[3 / 50]   Val: Loss = 0.19310, Accuracy = 93.49%: 100%|██████████| 13/13 [00:00<00:00, 86.10it/s]\n",
            "[4 / 50] Train: Loss = 0.13887, Accuracy = 95.39%: 100%|██████████| 572/572 [00:05<00:00, 101.77it/s]\n",
            "[4 / 50]   Val: Loss = 0.17592, Accuracy = 93.89%: 100%|██████████| 13/13 [00:00<00:00, 86.30it/s]\n",
            "[5 / 50] Train: Loss = 0.10828, Accuracy = 96.37%: 100%|██████████| 572/572 [00:05<00:00, 102.72it/s]\n",
            "[5 / 50]   Val: Loss = 0.16601, Accuracy = 94.28%: 100%|██████████| 13/13 [00:00<00:00, 89.21it/s]\n",
            "[6 / 50] Train: Loss = 0.08629, Accuracy = 97.12%: 100%|██████████| 572/572 [00:05<00:00, 101.98it/s]\n",
            "[6 / 50]   Val: Loss = 0.15623, Accuracy = 94.74%: 100%|██████████| 13/13 [00:00<00:00, 82.74it/s]\n",
            "[7 / 50] Train: Loss = 0.06950, Accuracy = 97.66%: 100%|██████████| 572/572 [00:05<00:00, 102.72it/s]\n",
            "[7 / 50]   Val: Loss = 0.16017, Accuracy = 94.82%: 100%|██████████| 13/13 [00:00<00:00, 88.72it/s]\n",
            "[8 / 50] Train: Loss = 0.05636, Accuracy = 98.12%: 100%|██████████| 572/572 [00:05<00:00, 102.86it/s]\n",
            "[8 / 50]   Val: Loss = 0.16221, Accuracy = 94.75%: 100%|██████████| 13/13 [00:00<00:00, 86.40it/s]\n",
            "[9 / 50] Train: Loss = 0.04637, Accuracy = 98.45%: 100%|██████████| 572/572 [00:05<00:00, 102.16it/s]\n",
            "[9 / 50]   Val: Loss = 0.16686, Accuracy = 94.85%: 100%|██████████| 13/13 [00:00<00:00, 88.66it/s]\n",
            "[10 / 50] Train: Loss = 0.03777, Accuracy = 98.76%: 100%|██████████| 572/572 [00:05<00:00, 102.33it/s]\n",
            "[10 / 50]   Val: Loss = 0.17842, Accuracy = 94.71%: 100%|██████████| 13/13 [00:00<00:00, 88.89it/s]\n",
            "[11 / 50] Train: Loss = 0.03098, Accuracy = 99.00%: 100%|██████████| 572/572 [00:05<00:00, 102.82it/s]\n",
            "[11 / 50]   Val: Loss = 0.18510, Accuracy = 94.74%: 100%|██████████| 13/13 [00:00<00:00, 81.80it/s]\n",
            "[12 / 50] Train: Loss = 0.02551, Accuracy = 99.18%: 100%|██████████| 572/572 [00:05<00:00, 103.09it/s]\n",
            "[12 / 50]   Val: Loss = 0.19199, Accuracy = 94.86%: 100%|██████████| 13/13 [00:00<00:00, 86.89it/s]\n",
            "[13 / 50] Train: Loss = 0.02062, Accuracy = 99.35%: 100%|██████████| 572/572 [00:05<00:00, 102.65it/s]\n",
            "[13 / 50]   Val: Loss = 0.21908, Accuracy = 94.56%: 100%|██████████| 13/13 [00:00<00:00, 84.79it/s]\n",
            "[14 / 50] Train: Loss = 0.01680, Accuracy = 99.48%: 100%|██████████| 572/572 [00:05<00:00, 102.22it/s]\n",
            "[14 / 50]   Val: Loss = 0.21481, Accuracy = 94.68%: 100%|██████████| 13/13 [00:00<00:00, 83.13it/s]\n",
            "[15 / 50] Train: Loss = 0.01402, Accuracy = 99.57%: 100%|██████████| 572/572 [00:05<00:00, 101.44it/s]\n",
            "[15 / 50]   Val: Loss = 0.22852, Accuracy = 94.62%: 100%|██████████| 13/13 [00:00<00:00, 85.57it/s]\n",
            "[16 / 50] Train: Loss = 0.01123, Accuracy = 99.67%: 100%|██████████| 572/572 [00:05<00:00, 101.66it/s]\n",
            "[16 / 50]   Val: Loss = 0.24491, Accuracy = 94.57%: 100%|██████████| 13/13 [00:00<00:00, 82.71it/s]\n",
            "[17 / 50] Train: Loss = 0.00941, Accuracy = 99.72%: 100%|██████████| 572/572 [00:05<00:00, 102.94it/s]\n",
            "[17 / 50]   Val: Loss = 0.25723, Accuracy = 94.51%: 100%|██████████| 13/13 [00:00<00:00, 83.46it/s]\n",
            "[18 / 50] Train: Loss = 0.00822, Accuracy = 99.76%: 100%|██████████| 572/572 [00:05<00:00, 102.33it/s]\n",
            "[18 / 50]   Val: Loss = 0.26504, Accuracy = 94.52%: 100%|██████████| 13/13 [00:00<00:00, 88.84it/s]\n",
            "[19 / 50] Train: Loss = 0.00749, Accuracy = 99.78%: 100%|██████████| 572/572 [00:05<00:00, 102.70it/s]\n",
            "[19 / 50]   Val: Loss = 0.26958, Accuracy = 94.59%: 100%|██████████| 13/13 [00:00<00:00, 81.52it/s]\n",
            "[20 / 50] Train: Loss = 0.00643, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 102.71it/s]\n",
            "[20 / 50]   Val: Loss = 0.28433, Accuracy = 94.48%: 100%|██████████| 13/13 [00:00<00:00, 89.40it/s]\n",
            "[21 / 50] Train: Loss = 0.00628, Accuracy = 99.80%: 100%|██████████| 572/572 [00:05<00:00, 102.76it/s]\n",
            "[21 / 50]   Val: Loss = 0.28675, Accuracy = 94.57%: 100%|██████████| 13/13 [00:00<00:00, 86.27it/s]\n",
            "[22 / 50] Train: Loss = 0.00571, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 102.98it/s]\n",
            "[22 / 50]   Val: Loss = 0.29572, Accuracy = 94.55%: 100%|██████████| 13/13 [00:00<00:00, 87.79it/s]\n",
            "[23 / 50] Train: Loss = 0.00556, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 102.13it/s]\n",
            "[23 / 50]   Val: Loss = 0.29893, Accuracy = 94.58%: 100%|██████████| 13/13 [00:00<00:00, 88.07it/s]\n",
            "[24 / 50] Train: Loss = 0.00518, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 102.45it/s]\n",
            "[24 / 50]   Val: Loss = 0.30354, Accuracy = 94.55%: 100%|██████████| 13/13 [00:00<00:00, 88.04it/s]\n",
            "[25 / 50] Train: Loss = 0.00545, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 102.49it/s]\n",
            "[25 / 50]   Val: Loss = 0.30896, Accuracy = 94.63%: 100%|██████████| 13/13 [00:00<00:00, 85.97it/s]\n",
            "[26 / 50] Train: Loss = 0.00547, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 102.36it/s]\n",
            "[26 / 50]   Val: Loss = 0.30968, Accuracy = 94.74%: 100%|██████████| 13/13 [00:00<00:00, 83.59it/s]\n",
            "[27 / 50] Train: Loss = 0.00548, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 102.39it/s]\n",
            "[27 / 50]   Val: Loss = 0.31606, Accuracy = 94.53%: 100%|██████████| 13/13 [00:00<00:00, 84.61it/s]\n",
            "[28 / 50] Train: Loss = 0.00467, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 103.00it/s]\n",
            "[28 / 50]   Val: Loss = 0.31550, Accuracy = 94.65%: 100%|██████████| 13/13 [00:00<00:00, 85.29it/s]\n",
            "[29 / 50] Train: Loss = 0.00437, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 101.82it/s]\n",
            "[29 / 50]   Val: Loss = 0.31667, Accuracy = 94.73%: 100%|██████████| 13/13 [00:00<00:00, 85.93it/s]\n",
            "[30 / 50] Train: Loss = 0.00448, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 101.64it/s]\n",
            "[30 / 50]   Val: Loss = 0.33979, Accuracy = 94.41%: 100%|██████████| 13/13 [00:00<00:00, 85.93it/s]\n",
            "[31 / 50] Train: Loss = 0.00456, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 101.84it/s]\n",
            "[31 / 50]   Val: Loss = 0.34083, Accuracy = 94.34%: 100%|██████████| 13/13 [00:00<00:00, 87.99it/s]\n",
            "[32 / 50] Train: Loss = 0.00585, Accuracy = 99.79%: 100%|██████████| 572/572 [00:05<00:00, 102.63it/s]\n",
            "[32 / 50]   Val: Loss = 0.34023, Accuracy = 94.55%: 100%|██████████| 13/13 [00:00<00:00, 84.66it/s]\n",
            "[33 / 50] Train: Loss = 0.00475, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 101.99it/s]\n",
            "[33 / 50]   Val: Loss = 0.33906, Accuracy = 94.66%: 100%|██████████| 13/13 [00:00<00:00, 86.94it/s]\n",
            "[34 / 50] Train: Loss = 0.00426, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 102.54it/s]\n",
            "[34 / 50]   Val: Loss = 0.34228, Accuracy = 94.57%: 100%|██████████| 13/13 [00:00<00:00, 88.07it/s]\n",
            "[35 / 50] Train: Loss = 0.00409, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 102.61it/s]\n",
            "[35 / 50]   Val: Loss = 0.33159, Accuracy = 94.75%: 100%|██████████| 13/13 [00:00<00:00, 81.18it/s]\n",
            "[36 / 50] Train: Loss = 0.00402, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 101.68it/s]\n",
            "[36 / 50]   Val: Loss = 0.33376, Accuracy = 94.70%: 100%|██████████| 13/13 [00:00<00:00, 88.23it/s]\n",
            "[37 / 50] Train: Loss = 0.00405, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 102.89it/s]\n",
            "[37 / 50]   Val: Loss = 0.34254, Accuracy = 94.56%: 100%|██████████| 13/13 [00:00<00:00, 88.22it/s]\n",
            "[38 / 50] Train: Loss = 0.00493, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 102.95it/s]\n",
            "[38 / 50]   Val: Loss = 0.33936, Accuracy = 94.65%: 100%|██████████| 13/13 [00:00<00:00, 83.62it/s]\n",
            "[39 / 50] Train: Loss = 0.00574, Accuracy = 99.79%: 100%|██████████| 572/572 [00:05<00:00, 102.62it/s]\n",
            "[39 / 50]   Val: Loss = 0.32719, Accuracy = 94.76%: 100%|██████████| 13/13 [00:00<00:00, 85.35it/s]\n",
            "[40 / 50] Train: Loss = 0.00414, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 101.85it/s]\n",
            "[40 / 50]   Val: Loss = 0.33598, Accuracy = 94.76%: 100%|██████████| 13/13 [00:00<00:00, 81.78it/s]\n",
            "[41 / 50] Train: Loss = 0.00386, Accuracy = 99.85%: 100%|██████████| 572/572 [00:05<00:00, 101.00it/s]\n",
            "[41 / 50]   Val: Loss = 0.33745, Accuracy = 94.72%: 100%|██████████| 13/13 [00:00<00:00, 85.36it/s]\n",
            "[42 / 50] Train: Loss = 0.00392, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 100.99it/s]\n",
            "[42 / 50]   Val: Loss = 0.33893, Accuracy = 94.75%: 100%|██████████| 13/13 [00:00<00:00, 88.00it/s]\n",
            "[43 / 50] Train: Loss = 0.00395, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 101.71it/s]\n",
            "[43 / 50]   Val: Loss = 0.34042, Accuracy = 94.76%: 100%|██████████| 13/13 [00:00<00:00, 88.24it/s]\n",
            "[44 / 50] Train: Loss = 0.00403, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 102.19it/s]\n",
            "[44 / 50]   Val: Loss = 0.34231, Accuracy = 94.63%: 100%|██████████| 13/13 [00:00<00:00, 89.03it/s]\n",
            "[45 / 50] Train: Loss = 0.00503, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 102.59it/s]\n",
            "[45 / 50]   Val: Loss = 0.33266, Accuracy = 94.67%: 100%|██████████| 13/13 [00:00<00:00, 84.95it/s]\n",
            "[46 / 50] Train: Loss = 0.00441, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 102.44it/s]\n",
            "[46 / 50]   Val: Loss = 0.34355, Accuracy = 94.66%: 100%|██████████| 13/13 [00:00<00:00, 88.12it/s]\n",
            "[47 / 50] Train: Loss = 0.00386, Accuracy = 99.85%: 100%|██████████| 572/572 [00:05<00:00, 102.12it/s]\n",
            "[47 / 50]   Val: Loss = 0.33947, Accuracy = 94.75%: 100%|██████████| 13/13 [00:00<00:00, 87.80it/s]\n",
            "[48 / 50] Train: Loss = 0.00374, Accuracy = 99.85%: 100%|██████████| 572/572 [00:05<00:00, 101.66it/s]\n",
            "[48 / 50]   Val: Loss = 0.33761, Accuracy = 94.83%: 100%|██████████| 13/13 [00:00<00:00, 85.80it/s]\n",
            "[49 / 50] Train: Loss = 0.00371, Accuracy = 99.85%: 100%|██████████| 572/572 [00:05<00:00, 102.65it/s]\n",
            "[49 / 50]   Val: Loss = 0.33594, Accuracy = 94.82%: 100%|██████████| 13/13 [00:00<00:00, 88.25it/s]\n",
            "[50 / 50] Train: Loss = 0.00380, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 102.57it/s]\n",
            "[50 / 50]   Val: Loss = 0.34632, Accuracy = 94.69%: 100%|██████████| 13/13 [00:00<00:00, 88.86it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98wr38_rw55D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "e0132296-cbf7-4c26-fb4a-54fed15b7819"
      },
      "source": [
        "test_loss, test_acc = do_epoch(model, criterion, (X_test, y_test), 512)\n",
        "print(f'\\n Test accuracy = {test_acc}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Loss = 0.34702, Accuracy = 94.77%: 100%|██████████| 28/28 [00:00<00:00, 89.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Test accuracy = 0.9477052628679825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqUQ0BaNk1R6",
        "colab_type": "text"
      },
      "source": [
        "Лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egJ-9sqK5lPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bidirectional(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
        "        self.linear = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding = self.embedding(inputs)\n",
        "        lstm_res, _  = self.lstm(embedding)\n",
        "        result = self.linear(lstm_res)\n",
        "\n",
        "        return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlj0vF7Rol5k",
        "colab_type": "text"
      },
      "source": [
        "Тренируем:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4hH9Ss8nzGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "79c7bcd8-56e5-4880-a887-2f10f2254664"
      },
      "source": [
        "bidirectional_model = Bidirectional(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0).cuda()\n",
        "optimizer = optim.Adam(bidirectional_model.parameters())\n",
        "\n",
        "fit(bidirectional_model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 20] Train: Loss = 0.55285, Accuracy = 82.66%: 100%|██████████| 572/572 [00:06<00:00, 89.09it/s]\n",
            "[1 / 20]   Val: Loss = 0.27048, Accuracy = 91.15%: 100%|██████████| 13/13 [00:00<00:00, 70.91it/s]\n",
            "[2 / 20] Train: Loss = 0.20197, Accuracy = 93.59%: 100%|██████████| 572/572 [00:06<00:00, 89.06it/s]\n",
            "[2 / 20]   Val: Loss = 0.17726, Accuracy = 94.34%: 100%|██████████| 13/13 [00:00<00:00, 72.45it/s]\n",
            "[3 / 20] Train: Loss = 0.12832, Accuracy = 96.04%: 100%|██████████| 572/572 [00:06<00:00, 89.09it/s]\n",
            "[3 / 20]   Val: Loss = 0.14131, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 72.59it/s]\n",
            "[4 / 20] Train: Loss = 0.08782, Accuracy = 97.32%: 100%|██████████| 572/572 [00:06<00:00, 89.09it/s]\n",
            "[4 / 20]   Val: Loss = 0.12657, Accuracy = 95.88%: 100%|██████████| 13/13 [00:00<00:00, 72.83it/s]\n",
            "[5 / 20] Train: Loss = 0.06080, Accuracy = 98.18%: 100%|██████████| 572/572 [00:06<00:00, 88.11it/s]\n",
            "[5 / 20]   Val: Loss = 0.12438, Accuracy = 96.01%: 100%|██████████| 13/13 [00:00<00:00, 70.72it/s]\n",
            "[6 / 20] Train: Loss = 0.04137, Accuracy = 98.81%: 100%|██████████| 572/572 [00:06<00:00, 84.86it/s]\n",
            "[6 / 20]   Val: Loss = 0.12062, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 69.50it/s]\n",
            "[7 / 20] Train: Loss = 0.02774, Accuracy = 99.24%: 100%|██████████| 572/572 [00:06<00:00, 87.82it/s]\n",
            "[7 / 20]   Val: Loss = 0.12676, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 71.37it/s]\n",
            "[8 / 20] Train: Loss = 0.01801, Accuracy = 99.54%: 100%|██████████| 572/572 [00:06<00:00, 88.97it/s]\n",
            "[8 / 20]   Val: Loss = 0.13288, Accuracy = 96.34%: 100%|██████████| 13/13 [00:00<00:00, 69.65it/s]\n",
            "[9 / 20] Train: Loss = 0.01155, Accuracy = 99.73%: 100%|██████████| 572/572 [00:06<00:00, 88.88it/s]\n",
            "[9 / 20]   Val: Loss = 0.13387, Accuracy = 96.47%: 100%|██████████| 13/13 [00:00<00:00, 70.82it/s]\n",
            "[10 / 20] Train: Loss = 0.00721, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 89.13it/s]\n",
            "[10 / 20]   Val: Loss = 0.14735, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 70.48it/s]\n",
            "[11 / 20] Train: Loss = 0.00444, Accuracy = 99.93%: 100%|██████████| 572/572 [00:06<00:00, 88.89it/s]\n",
            "[11 / 20]   Val: Loss = 0.15651, Accuracy = 96.34%: 100%|██████████| 13/13 [00:00<00:00, 70.73it/s]\n",
            "[12 / 20] Train: Loss = 0.00281, Accuracy = 99.96%: 100%|██████████| 572/572 [00:06<00:00, 88.50it/s]\n",
            "[12 / 20]   Val: Loss = 0.15954, Accuracy = 96.41%: 100%|██████████| 13/13 [00:00<00:00, 71.40it/s]\n",
            "[13 / 20] Train: Loss = 0.00182, Accuracy = 99.98%: 100%|██████████| 572/572 [00:06<00:00, 89.45it/s]\n",
            "[13 / 20]   Val: Loss = 0.16622, Accuracy = 96.43%: 100%|██████████| 13/13 [00:00<00:00, 69.23it/s]\n",
            "[14 / 20] Train: Loss = 0.00149, Accuracy = 99.98%: 100%|██████████| 572/572 [00:06<00:00, 89.54it/s]\n",
            "[14 / 20]   Val: Loss = 0.17567, Accuracy = 96.43%: 100%|██████████| 13/13 [00:00<00:00, 71.05it/s]\n",
            "[15 / 20] Train: Loss = 0.00110, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 89.31it/s]\n",
            "[15 / 20]   Val: Loss = 0.18435, Accuracy = 96.44%: 100%|██████████| 13/13 [00:00<00:00, 73.03it/s]\n",
            "[16 / 20] Train: Loss = 0.00160, Accuracy = 99.97%: 100%|██████████| 572/572 [00:06<00:00, 88.67it/s]\n",
            "[16 / 20]   Val: Loss = 0.19417, Accuracy = 96.40%: 100%|██████████| 13/13 [00:00<00:00, 71.36it/s]\n",
            "[17 / 20] Train: Loss = 0.00362, Accuracy = 99.90%: 100%|██████████| 572/572 [00:06<00:00, 89.30it/s]\n",
            "[17 / 20]   Val: Loss = 0.19718, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 72.24it/s]\n",
            "[18 / 20] Train: Loss = 0.00124, Accuracy = 99.98%: 100%|██████████| 572/572 [00:06<00:00, 88.65it/s]\n",
            "[18 / 20]   Val: Loss = 0.19067, Accuracy = 96.58%: 100%|██████████| 13/13 [00:00<00:00, 67.84it/s]\n",
            "[19 / 20] Train: Loss = 0.00037, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 88.63it/s]\n",
            "[19 / 20]   Val: Loss = 0.19184, Accuracy = 96.61%: 100%|██████████| 13/13 [00:00<00:00, 69.50it/s]\n",
            "[20 / 20] Train: Loss = 0.00019, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.22it/s]\n",
            "[20 / 20]   Val: Loss = 0.19512, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 72.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqbNFOF8or5O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "3639fc9d-5738-4eeb-fc28-89ef101f0501"
      },
      "source": [
        "b_test_loss, b_test_acc = do_epoch(bidirectional_model, criterion, (X_test, y_test), 512)\n",
        "print(f' \\n Test bidirectional accuracy = {b_test_acc}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Loss = 0.19635, Accuracy = 96.57%: 100%|██████████| 28/28 [00:00<00:00, 71.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Test bidirectional accuracy = 0.9657002984827985\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdqr-L00tPp4",
        "colab_type": "text"
      },
      "source": [
        "Стало лучше!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5f83bd11-7d6b-4cde-c87c-f6c479e4a849"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "17a47280-5586-453e-a41b-5c81f198fb78"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
        "        self.lstm = nn.LSTM(self.embedding.embedding_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
        "        self.linear = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding = self.embedding(inputs)\n",
        "        lstm_res, _  = self.lstm(embedding)\n",
        "        result = self.linear(lstm_res)\n",
        "\n",
        "        return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "1e392c87-ad2c-4596-8f30-a2c96819ba06"
      },
      "source": [
        "prtr_model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=torch.FloatTensor(embeddings),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(prtr_model.parameters())\n",
        "\n",
        "fit(prtr_model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=15,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 15] Train: Loss = 0.57192, Accuracy = 83.41%: 100%|██████████| 572/572 [00:05<00:00, 110.37it/s]\n",
            "[1 / 15]   Val: Loss = 0.25937, Accuracy = 92.39%: 100%|██████████| 13/13 [00:00<00:00, 90.75it/s]\n",
            "[2 / 15] Train: Loss = 0.19174, Accuracy = 94.32%: 100%|██████████| 572/572 [00:05<00:00, 114.08it/s]\n",
            "[2 / 15]   Val: Loss = 0.17705, Accuracy = 94.70%: 100%|██████████| 13/13 [00:00<00:00, 92.83it/s]\n",
            "[3 / 15] Train: Loss = 0.13659, Accuracy = 95.90%: 100%|██████████| 572/572 [00:04<00:00, 114.68it/s]\n",
            "[3 / 15]   Val: Loss = 0.14158, Accuracy = 95.65%: 100%|██████████| 13/13 [00:00<00:00, 86.74it/s]\n",
            "[4 / 15] Train: Loss = 0.10959, Accuracy = 96.71%: 100%|██████████| 572/572 [00:05<00:00, 114.36it/s]\n",
            "[4 / 15]   Val: Loss = 0.12342, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 86.64it/s]\n",
            "[5 / 15] Train: Loss = 0.09347, Accuracy = 97.15%: 100%|██████████| 572/572 [00:04<00:00, 115.60it/s]\n",
            "[5 / 15]   Val: Loss = 0.11525, Accuracy = 96.36%: 100%|██████████| 13/13 [00:00<00:00, 88.93it/s]\n",
            "[6 / 15] Train: Loss = 0.08296, Accuracy = 97.46%: 100%|██████████| 572/572 [00:05<00:00, 113.44it/s]\n",
            "[6 / 15]   Val: Loss = 0.10628, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 88.36it/s]\n",
            "[7 / 15] Train: Loss = 0.07535, Accuracy = 97.68%: 100%|██████████| 572/572 [00:05<00:00, 113.36it/s]\n",
            "[7 / 15]   Val: Loss = 0.10250, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 88.04it/s]\n",
            "[8 / 15] Train: Loss = 0.06951, Accuracy = 97.86%: 100%|██████████| 572/572 [00:05<00:00, 113.84it/s]\n",
            "[8 / 15]   Val: Loss = 0.09967, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 89.01it/s]\n",
            "[9 / 15] Train: Loss = 0.06465, Accuracy = 98.00%: 100%|██████████| 572/572 [00:05<00:00, 114.25it/s]\n",
            "[9 / 15]   Val: Loss = 0.09865, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 87.30it/s]\n",
            "[10 / 15] Train: Loss = 0.06061, Accuracy = 98.11%: 100%|██████████| 572/572 [00:05<00:00, 112.80it/s]\n",
            "[10 / 15]   Val: Loss = 0.09547, Accuracy = 96.95%: 100%|██████████| 13/13 [00:00<00:00, 88.85it/s]\n",
            "[11 / 15] Train: Loss = 0.05702, Accuracy = 98.22%: 100%|██████████| 572/572 [00:04<00:00, 114.50it/s]\n",
            "[11 / 15]   Val: Loss = 0.09589, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 90.15it/s]\n",
            "[12 / 15] Train: Loss = 0.05385, Accuracy = 98.31%: 100%|██████████| 572/572 [00:05<00:00, 113.24it/s]\n",
            "[12 / 15]   Val: Loss = 0.09669, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 86.20it/s]\n",
            "[13 / 15] Train: Loss = 0.05105, Accuracy = 98.40%: 100%|██████████| 572/572 [00:04<00:00, 114.78it/s]\n",
            "[13 / 15]   Val: Loss = 0.09806, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 85.31it/s]\n",
            "[14 / 15] Train: Loss = 0.04840, Accuracy = 98.48%: 100%|██████████| 572/572 [00:05<00:00, 113.37it/s]\n",
            "[14 / 15]   Val: Loss = 0.09337, Accuracy = 97.06%: 100%|██████████| 13/13 [00:00<00:00, 88.96it/s]\n",
            "[15 / 15] Train: Loss = 0.04616, Accuracy = 98.56%: 100%|██████████| 572/572 [00:04<00:00, 115.03it/s]\n",
            "[15 / 15]   Val: Loss = 0.09298, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 89.06it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "78191020-d5a7-4b5a-f4e1-60a9561b52d2"
      },
      "source": [
        "prtr_test_loss, prtr_test_acc = do_epoch(prtr_model, criterion, (X_test, y_test), 512)\n",
        "print(f' \\n Tess accuracy for pretrained model = {prtr_test_acc}')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Loss = 0.09281, Accuracy = 97.08%: 100%|██████████| 28/28 [00:00<00:00, 88.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Tess accuracy for pretrained model = 0.9708264909508416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11bD8aW_z9DW",
        "colab_type": "text"
      },
      "source": [
        "Лучшая модель!"
      ]
    }
  ]
}